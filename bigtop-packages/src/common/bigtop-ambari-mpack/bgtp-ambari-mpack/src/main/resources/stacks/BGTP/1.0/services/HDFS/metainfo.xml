<?xml version="1.0"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<metainfo xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <schemaVersion>2.0</schemaVersion>
  <services>
    <service>
      <name>HDFS</name>
      <serviceType>HDFS</serviceType> <!-- This tag is used only for main fileSystem service. It sets filesystem schema for ambari -->
      <displayName>HDFS</displayName>
      <comment>Apache Hadoop Distributed File System</comment>
      <version>Bigtop+3.2</version>

      <components>
        <component>
          <name>NAMENODE</name>
          <displayName>NameNode</displayName>
          <category>MASTER</category>
          <cardinality>1+</cardinality>
          <versionAdvertised>true</versionAdvertised>
          <reassignAllowed>true</reassignAllowed>
          <dependencies>
            <dependency>
              <name>HDFS/ZKFC</name>
              <scope>cluster</scope>
              <auto-deploy>
                <enabled>false</enabled>
              </auto-deploy>
              <conditions>
                <condition xsi:type="propertyExists">
                  <configType>hdfs-site</configType>
                  <property>dfs.nameservices</property>
                </condition>
              </conditions>
            </dependency>
            <dependency>
              <name>ZOOKEEPER/ZOOKEEPER_SERVER</name>
              <scope>cluster</scope>
              <auto-deploy>
                <enabled>false</enabled>
              </auto-deploy>
              <conditions>
                <condition xsi:type="propertyExists">
                  <configType>hdfs-site</configType>
                  <property>dfs.nameservices</property>
                </condition>
              </conditions>
            </dependency>
            <dependency>
              <name>HDFS/JOURNALNODE</name>
              <scope>cluster</scope>
              <auto-deploy>
                <enabled>false</enabled>
              </auto-deploy>
              <conditions>
                <condition xsi:type="propertyExists">
                  <configType>hdfs-site</configType>
                  <property>dfs.nameservices</property>
                </condition>
              </conditions>
            </dependency>
          </dependencies>
          <commandScript>
            <script>scripts/namenode.py</script>
            <scriptType>PYTHON</scriptType>
            <timeout>1800</timeout>
          </commandScript>
          <logs>
            <log>
              <logId>hdfs_namenode</logId>
              <primary>true</primary>
            </log>
            <log>
              <logId>hdfs_audit</logId>
            </log>
          </logs>
          <customCommands>
            <customCommand>
              <name>DECOMMISSION</name>
              <commandScript>
                <script>scripts/namenode.py</script>
                <scriptType>PYTHON</scriptType>
                <timeout>600</timeout>
              </commandScript>
            </customCommand>
            <customCommand>
              <name>REBALANCEHDFS</name>
              <background>true</background>
              <commandScript>
                <script>scripts/namenode.py</script>
                <scriptType>PYTHON</scriptType>
              </commandScript>
            </customCommand>
            <customCommand>
              <name>BOOTSTRAP_STANDBY</name>
              <commandScript>
                <script>scripts/namenode.py</script>
                <scriptType>PYTHON</scriptType>
              </commandScript>
              <hidden>true</hidden>
            </customCommand>
            <customCommand>
              <name>FORMAT</name>
              <commandScript>
                <script>scripts/namenode.py</script>
                <scriptType>PYTHON</scriptType>
              </commandScript>
              <hidden>true</hidden>
            </customCommand>
            <customCommand>
              <name>REFRESH_NODES</name>
              <commandScript>
                <script>scripts/namenode.py</script>
                <scriptType>PYTHON</scriptType>
                <timeout>600</timeout>
              </commandScript>
            </customCommand>
          </customCommands>
        </component>

        <component>
          <name>DATANODE</name>
          <displayName>DataNode</displayName>
          <category>SLAVE</category>
          <cardinality>1+</cardinality>
          <versionAdvertised>true</versionAdvertised>
          <decommissionAllowed>true</decommissionAllowed>
          <commandScript>
            <script>scripts/datanode.py</script>
            <scriptType>PYTHON</scriptType>
            <timeout>1200</timeout>
          </commandScript>
          <bulkCommands>
            <displayName>DataNodes</displayName>
            <!-- Used by decommission and recommission -->
            <masterComponent>NAMENODE</masterComponent>
          </bulkCommands>
          <logs>
            <log>
              <logId>hdfs_datanode</logId>
              <primary>true</primary>
            </log>
          </logs>
        </component>

        <component>
          <name>SECONDARY_NAMENODE</name>
          <displayName>SNameNode</displayName>
          <!-- TODO:  cardinality is conditional on HA usage -->
          <cardinality>1</cardinality>
          <versionAdvertised>true</versionAdvertised>
          <reassignAllowed>true</reassignAllowed>
          <category>MASTER</category>
          <commandScript>
            <script>scripts/snamenode.py</script>
            <scriptType>PYTHON</scriptType>
            <timeout>1200</timeout>
          </commandScript>
          <logs>
            <log>
              <logId>hdfs_secondary_namenode</logId>
              <primary>true</primary>
            </log>
          </logs>
        </component>

        <component>
          <name>HDFS_CLIENT</name>
          <displayName>HDFS Client</displayName>
          <category>CLIENT</category>
          <componentType>HCFS_CLIENT</componentType>
          <cardinality>1+</cardinality>
          <versionAdvertised>true</versionAdvertised>
          <commandScript>
            <script>scripts/hdfs_client.py</script>
            <scriptType>PYTHON</scriptType>
            <timeout>1200</timeout>
          </commandScript>
          <configFiles>
            <configFile>
              <type>xml</type>
              <fileName>hdfs-site.xml</fileName>
              <dictionaryName>hdfs-site</dictionaryName>
            </configFile>
            <configFile>
              <type>xml</type>
              <fileName>core-site.xml</fileName>
              <dictionaryName>core-site</dictionaryName>
            </configFile>
            <configFile>
              <type>env</type>
              <fileName>log4j.properties</fileName>
              <dictionaryName>hdfs-log4j,yarn-log4j</dictionaryName>
            </configFile>
            <configFile>
              <type>env</type>
              <fileName>hadoop-env.sh</fileName>
              <dictionaryName>hadoop-env</dictionaryName>
            </configFile>
          </configFiles>
        </component>

        <component>
          <name>JOURNALNODE</name>
          <displayName>JournalNode</displayName>
          <category>SLAVE</category>
          <cardinality>0+</cardinality>
          <versionAdvertised>true</versionAdvertised>
          <commandScript>
            <script>scripts/journalnode.py</script>
            <scriptType>PYTHON</scriptType>
            <timeout>1200</timeout>
          </commandScript>
          <bulkCommands>
            <displayName>JournalNodes</displayName>
            <masterComponent>NAMENODE</masterComponent>
          </bulkCommands>
          <logs>
            <log>
              <logId>hdfs_journalnode</logId>
              <primary>true</primary>
            </log>
          </logs>
          <dependencies>
            <dependency>
              <name>HDFS/HDFS_CLIENT</name>
              <scope>host</scope>
              <auto-deploy>
                <enabled>true</enabled>
              </auto-deploy>
            </dependency>
          </dependencies>
        </component>

        <component>
          <name>ZKFC</name>
          <displayName>ZKFailoverController</displayName>
          <category>SLAVE</category>
          <!-- TODO: cardinality is conditional on HA topology -->
          <cardinality>0+</cardinality>
          <versionAdvertised>true</versionAdvertised>
          <commandScript>
            <script>scripts/zkfc_slave.py</script>
            <scriptType>PYTHON</scriptType>
            <timeout>1200</timeout>
          </commandScript>
          <logs>
            <log>
              <logId>hdfs_zkfc</logId>
              <primary>true</primary>
            </log>
          </logs>
          <customCommands>
            <customCommand>
              <name>FORMAT</name>
              <commandScript>
                <script>scripts/zkfc_slave.py</script>
                <scriptType>PYTHON</scriptType>
              </commandScript>
              <hidden>true</hidden>
            </customCommand>
          </customCommands>
        </component>

        <component>
          <name>NFS_GATEWAY</name>
          <displayName>NFSGateway</displayName>
          <cardinality>0+</cardinality>
          <versionAdvertised>true</versionAdvertised>
          <category>SLAVE</category>
          <commandScript>
            <script>scripts/nfsgateway.py</script>
            <scriptType>PYTHON</scriptType>
            <timeout>1200</timeout>
          </commandScript>
          <dependencies>
            <dependency>
              <name>HDFS/HDFS_CLIENT</name>
              <scope>host</scope>
              <auto-deploy>
                <enabled>true</enabled>
              </auto-deploy>
            </dependency>
          </dependencies>
          <logs>
            <log>
              <logId>hdfs_nfs3</logId>
              <primary>true</primary>
            </log>
          </logs>
        </component>
      </components>

      <osSpecifics>
        <osSpecific>
          <osFamily>any</osFamily>
          <packages>
            <package>
              <name>rpcbind</name>
              <condition>should_install_rpcbind</condition>
            </package>
          </packages>
        </osSpecific>

        <osSpecific>
          <osFamily>redhat7,amazonlinux2,redhat6,suse11</osFamily>
          <packages>
            <package>
              <name>hadoop</name>
            </package>
            <package>
              <name>hadoop-client</name>
            </package>
            <package>
              <name>snappy</name>
            </package>
            <package>
              <name>snappy-devel</name>
            </package>
            <package>
              <name>hadoop-libhdfs</name>
            </package>
            <package>
              <name>ranger-hdfs-plugin</name>
            </package>
          </packages>
        </osSpecific>

        <osSpecific>
          <osFamily>suse12</osFamily>
          <packages>
            <package>
              <name>hadoop</name>
            </package>
            <package>
              <name>hadoop-client</name>
            </package>
            <package>
              <name>snappy</name>
            </package>
            <package>
              <name>snappy-devel</name>
            </package>
            <package>
              <name>hadoop-libhdfs</name>
            </package>
            <package>
              <name>ranger-hdfs-plugin</name>
            </package>
          </packages>
        </osSpecific>

        <osSpecific>
          <osFamily>debian7,ubuntu12,ubuntu14</osFamily>
          <packages>
            <package>
              <name>hadoop-client</name>
            </package>
            <package>
              <name>hadoop-hdfs-datanode</name>
            </package>
            <package>
              <name>hadoop-hdfs-journalnode</name>
            </package>
            <package>
              <name>hadoop-hdfs-namenode</name>
            </package>
            <package>
              <name>hadoop-hdfs-secondarynamenode</name>
            </package>
            <package>
              <name>hadoop-hdfs-zkfc</name>
            </package>
            <package>
              <name>libsnappy1</name>
            </package>
            <package>
              <name>libsnappy-dev</name>
            </package>
            <package>
              <name>libhdfs0</name>
            </package>
            <package>
              <name>ranger-hdfs-plugin</name>
            </package>
          </packages>
        </osSpecific>
        <osSpecific>
          <osFamily>debian9,ubuntu16,ubuntu18</osFamily>
          <packages>
            <package>
              <name>hadoop-client</name>
            </package>
            <package>
              <name>hadoop-hdfs-datanode</name>
            </package>
            <package>
              <name>hadoop-hdfs-journalnode</name>
            </package>
            <package>
              <name>hadoop-hdfs-namenode</name>
            </package>
            <package>
              <name>hadoop-hdfs-secondarynamenode</name>
            </package>
            <package>
              <name>hadoop-hdfs-zkfc</name>
            </package>
            <package>
              <name>libsnappy1v5</name>
            </package>
            <package>
              <name>libsnappy-dev</name>
            </package>
            <package>
              <name>libhdfs0</name>
            </package>
            <package>
              <name>ranger-hdfs-plugin</name>
            </package>
          </packages>
        </osSpecific>
      </osSpecifics>

      <commandScript>
        <script>scripts/service_check.py</script>
        <scriptType>PYTHON</scriptType>
        <timeout>300</timeout>
      </commandScript>

      <configuration-dependencies>
        <config-type>core-site</config-type>
        <config-type>viewfs-mount-table</config-type>
        <config-type>hdfs-site</config-type>
        <config-type>hadoop-env</config-type>
        <config-type>hadoop-policy</config-type>
        <config-type>hdfs-log4j</config-type>
        <config-type>ranger-hdfs-plugin-properties</config-type>
        <config-type>ssl-client</config-type>
        <config-type>ssl-server</config-type>
        <config-type>ranger-hdfs-audit</config-type>
        <config-type>ranger-hdfs-policymgr-ssl</config-type>
        <config-type>ranger-hdfs-security</config-type>
        <!-- <config-type>ams-ssl-client</config-type>
        <config-type>hadoop-metrics2.properties</config-type> -->
      </configuration-dependencies>
      <restartRequiredAfterRackChange>true</restartRequiredAfterRackChange>

      <sso>
        <supported>true</supported>
        <kerberosRequired>true</kerberosRequired>
        <ssoEnabledTest>
          {
          "or": [
          {
          "equals": [
          "hdfs-site/hadoop.http.authentication.type",
          "org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler"
          ]
          },
          {
          "equals": [
          "core-site/hadoop.http.authentication.type",
          "org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler"
          ]
          }
          ]
          }
        </ssoEnabledTest>
      </sso>

      <kerberosEnabledTest>
        {
        "or": [
        {
        "equals": [
        "hdfs-site/hadoop.security.authentication",
        "kerberos"
        ]
        },
        {
        "equals": [
        "core-site/hadoop.security.authentication",
        "kerberos"
        ]
        }
        ]
        }
      </kerberosEnabledTest>

      <quickLinksConfigurations>
        <quickLinksConfiguration>
          <fileName>quicklinks.json</fileName>
          <default>true</default>
        </quickLinksConfiguration>
      </quickLinksConfigurations>

      <themes>
        <theme>
          <fileName>theme.json</fileName>
          <default>true</default>
        </theme>
        <theme>
          <fileName>directories.json</fileName>
          <default>true</default>
        </theme>
      </themes>
    </service>
  </services>
</metainfo>
