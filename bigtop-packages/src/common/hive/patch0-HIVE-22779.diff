diff --git a/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java b/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java
index 4add29027d..130d98a144 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/stats/TestStatsUtils.java
@@ -26,12 +26,12 @@
 import java.lang.reflect.Modifier;
 import java.util.Set;
 
+import com.google.common.collect.Sets;
 import org.apache.commons.lang.reflect.FieldUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.plan.ColStatistics.Range;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.junit.Test;
-import org.spark_project.guava.collect.Sets;
 
 public class TestStatsUtils {
 
diff --git a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java b/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java
index 64a4b86042..d27b1700cd 100644
--- a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java
+++ b/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java
@@ -47,8 +47,8 @@ public ShuffleWriteMetrics(
   }
 
   public ShuffleWriteMetrics(TaskMetrics metrics) {
-    this(metrics.shuffleWriteMetrics().shuffleBytesWritten(),
-      metrics.shuffleWriteMetrics().shuffleWriteTime());
+    this(metrics.shuffleWriteMetrics().bytesWritten(),
+      metrics.shuffleWriteMetrics().writeTime());
   }
 
 }
diff --git a/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java b/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java
index d0eb1fa446..bb494b6fb4 100644
--- a/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java
+++ b/spark-client/src/main/java/org/apache/hive/spark/counter/SparkCounter.java
@@ -19,15 +19,14 @@
 
 import java.io.Serializable;
 
-import org.apache.spark.Accumulator;
-import org.apache.spark.AccumulatorParam;
 import org.apache.spark.api.java.JavaSparkContext;
+import org.apache.spark.util.LongAccumulator;
 
 public class SparkCounter implements Serializable {
 
   private String name;
   private String displayName;
-  private Accumulator<Long> accumulator;
+  private LongAccumulator accumulator;
 
   // Values of accumulators can only be read on the SparkContext side. This field is used when
   // creating a snapshot to be sent to the RSC client.
@@ -55,9 +54,16 @@ public SparkCounter(
 
     this.name = name;
     this.displayName = displayName;
-    LongAccumulatorParam longParam = new LongAccumulatorParam();
     String accumulatorName = groupName + "_" + name;
-    this.accumulator = sparkContext.accumulator(initValue, accumulatorName, longParam);
+    this.accumulator = createAccumulator(sparkContext, accumulatorName, initValue);
+  }
+
+  private LongAccumulator createAccumulator(
+      JavaSparkContext sparkContext, String accumulatorName, long initValue) {
+    LongAccumulator accumulator = new LongAccumulator();
+    accumulator.setValue(initValue);
+    sparkContext.sc().register(accumulator, accumulatorName);
+    return accumulator;
   }
 
   public long getValue() {
@@ -87,23 +93,4 @@ public void setDisplayName(String displayName) {
   SparkCounter snapshot() {
     return new SparkCounter(name, displayName, accumulator.value());
   }
-
-  class LongAccumulatorParam implements AccumulatorParam<Long> {
-
-    @Override
-    public Long addAccumulator(Long t1, Long t2) {
-      return t1 + t2;
-    }
-
-    @Override
-    public Long addInPlace(Long r1, Long r2) {
-      return r1 + r2;
-    }
-
-    @Override
-    public Long zero(Long initialValue) {
-      return 0L;
-    }
-  }
-
 }
