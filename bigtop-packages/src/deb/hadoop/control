# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

Source: hadoop
Section: misc
Priority: extra
Maintainer: Bigtop <bigtop-dev@incubator.apache.org>
Build-Depends: debhelper (>= 6), ant, ant-optional, liblzo2-dev, python, libzip-dev,automake, autoconf (>= 2.61), sharutils, g++ (>= 4), git-core, libfuse-dev, libssl-dev
Standards-Version: 3.8.0
Homepage: http://hadoop.apache.org/core/

Package: hadoop
Provides: hadoop
Architecture: all
Depends: ${shlibs:Depends}, ${misc:Depends}, adduser, bigtop-utils
Description: A software platform for processing vast amounts of data
 Hadoop is a software platform that lets one easily write and
 run applications that process vast amounts of data.
 .
 Here's what makes Hadoop especially useful:
  * Scalable: Hadoop can reliably store and process petabytes.
  * Economical: It distributes the data and processing across clusters
                of commonly available computers. These clusters can number
                into the thousands of nodes.
  * Efficient: By distributing the data, Hadoop can process it in parallel
               on the nodes where the data is located. This makes it
               extremely rapid.
  * Reliable: Hadoop automatically maintains multiple copies of data and
              automatically redeploys computing tasks based on failures.
 .
 Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
 MapReduce divides applications into many small blocks of work. HDFS creates
 multiple replicas of data blocks for reliability, placing them on compute
 nodes around the cluster. MapReduce can then process the data where it is
 located.

Package: hadoop-fuse
Provides: hadoop-fuse
Architecture: i386 amd64
Depends: ${shlibs:Depends}, hadoop (= ${source:Version}), libfuse2, fuse-utils
Enhances: hadoop
Description: HDFS exposed over a Filesystem in Userspace
 These projects (enumerated below) allow HDFS to be mounted (on most flavors 
 of Unix) as a standard file system using the mount command. Once mounted, the
  user can operate on an instance of hdfs using standard Unix utilities such 
 as 'ls', 'cd', 'cp', 'mkdir', 'find', 'grep', or use standard Posix libraries 
 like open, write, read, close from C, C++, Python, Ruby, Perl, Java, bash, etc.

Package: hadoop-doc
Provides: hadoop-doc
Architecture: all
Section: doc
Description: Documentation for Hadoop
 This package contains the Java Documentation for Hadoop and its relevant
 APIs.

Package: hadoop-source
Provides: hadoop-source
Architecture: all
Description: Source code for Hadoop
 This package contains the source code for Hadoop and its contrib modules.

Package: hadoop-conf-pseudo
Provides: hadoop-conf-pseudo
Architecture: all
Depends: hadoop (= ${source:Version}), hadoop-namenode (= ${source:Version}), hadoop-datanode (= ${source:Version}), hadoop-secondarynamenode (= ${source:Version}), hadoop-resourcemanager (= ${source:Version}), hadoop-nodemanager (= ${source:Version})
Description: Pseudo-distributed Hadoop configuration
 Contains configuration files for a "pseudo-distributed" Hadoop deployment.
 In this mode, each of the hadoop components runs as a separate Java process,
 but all on the same machine.

Package: hadoop-nodemanager
Provides: hadoop-nodemanager
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Node manager for Hadoop
 The NodeManager is the per-machine framework agent who is responsible for
 containers, monitoring their resource usage (cpu, memory, disk, network) and
 reporting the same to the ResourceManager/Scheduler.

Package: hadoop-resourcemanager
Provides: hadoop-resourcemanager
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Resource manager for Hadoop
 The resource manager manages the global assignment of compute resources to applications.

Package: hadoop-namenode
Provides: hadoop-namenode
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Name Node for Hadoop
 The Hadoop Distributed Filesystem (HDFS) requires one unique server, the
 namenode, which manages the block locations of files on the filesystem.

Package: hadoop-secondarynamenode
Provides: hadoop-secondarynamenode
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Secondary Name Node for Hadoop
 The Secondary Name Node is responsible for checkpointing file system images.
 It is _not_ a failover pair for the namenode, and may safely be run on the
 same machine.

Package: hadoop-datanode
Provides: hadoop-datanode
Architecture: all
Depends: hadoop (= ${source:Version})
Description: Data Node for Hadoop
 The Data Nodes in the Hadoop Cluster are responsible for serving up
 blocks of data over the network to Hadoop Distributed Filesystem
 (HDFS) clients.

Package: hadoop-pipes
Provides: hadoop-pipes
Architecture: any
Depends: hadoop (= ${source:Version})
Description: Interface to author Hadoop MapReduce jobs in C++
 Contains Hadoop Pipes, a library which allows Hadoop MapReduce jobs to be
 written in C++.

Package: libhdfs0
Architecture: any
Depends: hadoop (= ${source:Version}), ${shlibs:Depends}
Description: JNI Bindings to access Hadoop HDFS from C
 See http://wiki.apache.org/hadoop/LibHDFS

Package: libhdfs0-dev
Architecture: any
Section: libdevel
Depends: hadoop (= ${source:Version}), libhdfs0 (= ${binary:Version})
Description: Development support for libhdfs0
 Includes examples and header files for accessing HDFS from C
